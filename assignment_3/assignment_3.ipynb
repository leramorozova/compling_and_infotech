{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "1. Implement Logistic Regression with Stochastic Gradient Decent using numpy\n",
    "1. Implement Logistic Regression with early stopping using pytorch\n",
    "\n",
    "Additional readings:\n",
    "1. https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html\n",
    "1. https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "# retrieve dataset\n",
    "data = fetch_20newsgroups()\n",
    "\n",
    "\n",
    "X_train = data['data']\n",
    "y_train = data['target']\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df=0.5, min_df=10)\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "\n",
    "test_data = fetch_20newsgroups(subset='test')\n",
    "X_test = tfidf.transform(test_data['data'])\n",
    "y_test = test_data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Binary Logistic Regression\n",
    "$\\{(x_i, y_i)\\}_{i=1}^N$, $y \\in \\{0,1\\}$\n",
    "$$ z = Xw + b $$\n",
    "\n",
    "$$p(y=1 | x) = \\sigma(z) = \\frac 1 {1 + e^{-z}}$$\n",
    "\n",
    "$$ L_{batch} = - \\frac 1 {|batch|} \\sum_{i \\in batch}^N [ y_i \\log \\sigma(z_i) + (1 - y_i) \\log (1 - \\sigma(z_i)) ] + \\frac \\lambda 2 w^T w$$\n",
    "\n",
    "Stochastic Gradient Decent for logreg:\n",
    "1. init w ~ random N(0,1), b = 0\n",
    "1. for epoch = 1..n_epochs:\n",
    "    * shuffle dataset\n",
    "    * for every batch:\n",
    "        * $w^{(t)} \\leftarrow w^{(t-1)} - \\alpha \\nabla_{w} L_{batch}(w^{(t-1)},b^{(t-1)})$\n",
    "        * $b^{(t)} \\leftarrow b^{(t-1)} - \\alpha \\nabla_{b} L_{batch}(w^{(t-1)},b^{(t-1)})$\n",
    "        \n",
    "$w$ - weights  \n",
    "$b$ - biases  \n",
    "$\\alpha$ - learning rate\n",
    "\n",
    "Hint:\n",
    "$$\\nabla_w L = \\frac {\\partial L} {\\partial \\sigma} \\frac {\\partial \\sigma} {\\partial z} \\frac {\\partial z} {\\partial w} + \\frac {\\partial (\\frac \\lambda 2 w^T w)} {\\partial w} $$\n",
    "$$\\nabla_b L = \\frac {\\partial L} {\\partial \\sigma} \\frac {\\partial \\sigma} {\\partial z} \\frac {\\partial z} {\\partial b} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1064,), (708,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dataset for binary classification\n",
    "\n",
    "X_train_bin = X_train[y_train < 2]\n",
    "y_train_bin = y_train[y_train < 2]\n",
    "\n",
    "X_test_bin = X_test[y_test < 2]\n",
    "y_test_bin = y_test[y_test < 2]\n",
    "\n",
    "y_train_bin.shape, y_test_bin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc 0.9963575118259986\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LogRegNumpy(ClassifierMixin):\n",
    "    def __init__(self, llambda=1, lr=0.1, batch_size=32, n_epochs=100):\n",
    "        \"\"\"\n",
    "        llambda: regularization strength\n",
    "        lr: learning rate\n",
    "        \"\"\"\n",
    "        self.w = None\n",
    "        self.b = 0\n",
    "        self.llambda = llambda\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.history = []\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(pred):\n",
    "        return 1.0 / (1 + np.exp(-pred))\n",
    "    \n",
    "    def calculate_loss(self, y_pred, batch_X, batch_y):\n",
    "        loss = -batch_y * np.log(y_pred) - (1 - batch_y) * np.log(1 - y_pred)\n",
    "        return loss.sum() / batch_X.shape[0] + (self.llambda/2) * self.w.T.dot(self.w)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(42)\n",
    "        self.w = np.random.randn(X.shape[1])\n",
    "        self.b = 0\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            \n",
    "            # random permutation over indices of dataset\n",
    "            batch_indices = np.random.permutation(len(y))\n",
    "            \n",
    "            for j in range(0, len(y), self.batch_size):\n",
    "                batch_idx = batch_indices[j:j+self.batch_size]\n",
    "                batch_X = X[batch_idx]\n",
    "                batch_y = y[batch_idx]\n",
    "                \n",
    "                y_pred = self.predict_proba(batch_X)\n",
    "            \n",
    "                # forward pass\n",
    "                # <TODO> [1 point] calculate batch loss\n",
    "                loss = self.calculate_loss(y_pred, batch_X, batch_y)\n",
    "                dz = y_pred - batch_y\n",
    "                \n",
    "                # backward pass\n",
    "                # <TODO> [2 points] calculate batch gradients \n",
    "                grad_w = (1 / batch_X.shape[1]) * batch_X.T.dot(dz.T) + self.llambda * self.w\n",
    "                grad_b = (1 / batch_X.shape[1]) * np.sum(dz)\n",
    "\n",
    "                # SGD optimization step\n",
    "                # <TODO> [1 point]\n",
    "                self.w -= self.lr * grad_w\n",
    "                self.b -= self.lr * grad_b\n",
    "\n",
    "                self.history.append(loss)\n",
    "        \n",
    "        return self \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # <TODO> [1 point] calculate p(y=1 | x)\n",
    "        pred = X.dot(self.w) + self.b\n",
    "        return self.sigmoid(pred)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5).astype(np.int)\n",
    "    \n",
    "    \n",
    "model = LogRegNumpy(llambda=3, lr=0.01, batch_size=128, n_epochs=100)\n",
    "model.fit(X_train_bin, y_train_bin)\n",
    "print('auc', metrics.roc_auc_score(y_test_bin, model.predict_proba(X_test_bin)))\n",
    "\n",
    "plt.plot(np.arange(len(model.history)), model.history)\n",
    "plt.xlabel('iters')\n",
    "plt.ylabel('train loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [1 point] implement linear model $Z = XW + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super(LogRegNN, self).__init__()\n",
    "        \n",
    "        # create tensor of weights and tensor of biases\n",
    "        # initialize tensors from N(0,1) \n",
    "        # W has shape (n_features, n_classes)\n",
    "        # b has shape (n_classes,)\n",
    "        self.W = nn.Parameter(torch.randn(n_features, n_classes))\n",
    "        self.b = nn.Parameter(torch.randn(n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In this method we implement connections between neural network weights\n",
    "        x: batch feature matrix\n",
    "        returns: probability logits\n",
    "        \"\"\"\n",
    "        # <TODO> implement linear model \n",
    "        result = x @ self.W + self.b\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, batch_x, batch_y):\n",
    "    # set NN model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward pass\n",
    "    pred = model(batch_x)\n",
    "    # cross-entropy loss\n",
    "    loss = criterion(pred, batch_y)\n",
    "    # calculate gradients\n",
    "    loss.backward()\n",
    "    # make optimization step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # return batch loss\n",
    "    return loss.data.detach().item()\n",
    "\n",
    "def eval_batch(model, batch_x, batch_y):\n",
    "    # set NN model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # forward pass\n",
    "        pred = model(batch_x)\n",
    "        # cross-entropy loss\n",
    "        loss = criterion(pred, batch_y)\n",
    "\n",
    "    # return batch loss\n",
    "    return loss.data.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2 points] implement early stopping using early_stopping_patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def train(model, train_loader, valid_loader, n_epochs, early_stopping_patience=5):\n",
    "    \"\"\"\n",
    "    early_stopping_patience - number of consecutive epochs of growing validation loss to wait\n",
    "    \"\"\"\n",
    "    history = {'train': [], 'valid': []}\n",
    "\n",
    "    # <TODO> implement early stopping using early_stopping_patience\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_train_loss = 0\n",
    "        \n",
    "        # iterate over batches\n",
    "        for batch_x, batch_y in tqdm(train_loader, desc=f'epoch:{epoch}', leave=True):\n",
    "\n",
    "            loss = train_batch(model, batch_x, batch_y)\n",
    "            epoch_train_loss += loss\n",
    "\n",
    "        # average loss for epoch\n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        history['train'].append(epoch_train_loss)\n",
    "        \n",
    "        epoch_valid_loss = 0\n",
    "        for batch_x, batch_y in valid_loader:\n",
    "            loss = eval_batch(model, batch_x, batch_y)\n",
    "            epoch_valid_loss += loss\n",
    "            \n",
    "        epoch_valid_loss /= len(valid_loader)\n",
    "        print(f'train loss: {epoch_train_loss:.3f} valid loss:{epoch_valid_loss:.3f}')\n",
    "        history['valid'].append(epoch_valid_loss)\n",
    "        \n",
    "        # <TODO> implement early stopping using early_stopping_patience\n",
    "        if len(history['valid']) > early_stopping_patience:\n",
    "            continue\n",
    "            \n",
    "        if reduce(lambda x, y: x < y, history['valid'][-early_stopping_patience:]):\n",
    "            return history \n",
    "\n",
    "    return history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict hard labels [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x in test_loader:\n",
    "            # <TODO> predict hard labels\n",
    "            pred = model.forward(batch_x)\n",
    "            result.append(pred)\n",
    "    return np.concatenate(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader implements iteration over batches and shuffling\n",
    "\n",
    "L2 regularization coef is embedded into SGD.weight_decay: \n",
    "\n",
    "1. $w^{(t-1)} \\leftarrow w^{(t-2)} - \\alpha \\lambda w^{(t-2)} $  \n",
    "1. $w^{(t)} \\leftarrow w^{(t-1)} - \\alpha \\nabla_{w} L(w^{(t-1)}) $\n",
    "\n",
    "here   \n",
    "$\\alpha$ - learning rate  \n",
    "$\\lambda$ - weight-decay = L2 regularization coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bdc152578546d1b28b0966e1c6b88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch:0', max=36, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 3.748 valid loss:3.721\n"
     ]
    }
   ],
   "source": [
    "X_train2, X_valid2, y_train2, y_valid2 = train_test_split(X_train, y_train, stratify=y_train,\n",
    "                                                      shuffle=True, test_size=0.2, random_state=42)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_train2.todense()).float(), torch.tensor(y_train2).long()), \n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(torch.tensor(X_valid2.todense()).float(), torch.tensor(y_valid2).long()), \n",
    "                          batch_size=batch_size)\n",
    "test_loader = DataLoader(torch.tensor(X_test.todense()).float(), \n",
    "                          batch_size=batch_size)\n",
    "\n",
    "model = LogRegNN(X_train.shape[1], 20)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# L2 regularization is embeded in \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "\n",
    "history = train(model, train_loader, valid_loader, 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7778ba6978>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(history['train'])), history['train'], label='train')\n",
    "plt.plot(np.arange(len(history['valid'])), history['valid'], label='valid')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/denaas/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:84: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7947424322889007"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(model, test_loader)\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the cross-entropy loss of uniformly random guessing classifier for this task? [0.5 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the accuracy of constant prediction classifier for this task? [0.5 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
